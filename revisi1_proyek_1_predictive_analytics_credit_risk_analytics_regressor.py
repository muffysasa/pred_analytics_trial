# -*- coding: utf-8 -*-
"""Revisi1_Proyek 1 Predictive Analytics- Credit Risk Analytics Regressor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LdjZPsTu6nLIu8ECGzWSosgeUi4iXf3p

##Laporan Proyek 1 Predictive Analytics Machine Learning - Makrufiah Sakatri

### 1. Domain Project

Kredit bank merupakan penyediaan uang atau tagihan, berdasarkan persetujuan atau kesepakatan pinjam-meminjam antara bank dan debitur, yang mewajibkan pihak debitur untuk melunasi hutangnya setelah jangka waktu tertentu yang disepakati. Namun dalam aktivitas kredit bank terdapat resiko kredit yang disebabkan faktor dari debitur tersebut untuk menunda pelunasan hutang berujung melakukan gagal bayar, dimana hal tersebut berujung merugikan pihak bank [Resiko Kredit](https://www.ocbc.id/id/article/2022/02/24/risiko-kredit-adalah?_gl=1*1ihxzca*_gcl_au*MTIzMzU0NDI2MS4xNzI4Nzk5ODEy). Analisis resiko kresit pada Bank merupakan aktivitas penilaian kredit Bank terhadap debitur untuk menghindari kerugian bank terhadap debitur yang memiliki potensi resiko kredit [Analisis Resiko Kredit](https://www.ocbc.id/id/article/2022/11/15/analisis-kredit-adalah). Jurnal: [Jurnal Analisis Resiko Kredit](https://journal.metansi.unipol.ac.id/index.php/jurnalmetansi/article/view/87)

### 2. Business Understanding

**- Problem Statement**

Dalam dunia perbankan dan lembaga keuangan, pengelolaan risiko kredit merupakan aspek penting untuk mendapatkan profitabilitas suatu lembaga. Masalah umum pada kredit bank sebagian besar berikut:
1. Meningkatnya jumlah pinjaman yang diberikan kepada debitur dari berbagai latar belakang
2. Kurangnya penyesuaian kontrak kredit pada tanpa analisis profil debitur
3. Profitabilitas perusaahaan menurun karena banyaknya debitur yang gagal bayar dimana kurangnya bank untuk menargetkan analisis resiko kredit.

Berdasarkan latar belakang tersebut project ini memiliki batasan masalah sebagai berikut:
1. **Bagaimana langkah-langkah untuk menurunkan debitur yang berpotensi gagal bayar?**
  --> Melakukan metode sederhana CRISP-DM (Cross-Industry Standard Process for Data Mining) yaitu melakukan proses analitik pada data history/profil kreditur bank tersebut untuk predictive analytics pada skor nilai resiko kredit.
2. **Apa variabel pada profil debitur yang mempengaruhi debitur gagal bayar?**
  --> Menganalisis data dan mencari korelasi dari data history terhadap variabel skor resiko kredit
3. **Bagaimana cara membangun model Machine Learning sebagai solusi untuk analisis resiko kredit pada nasabah menggunakan data history profil debitur?**
  --> Analisis data dan membangun model Predictive Analytics dangan eksplorasi algoritma Machine Learning Regresi dengan melihat pada MSE, MAE dan R2 sebagai parameter algoritma model yang memiliki error paling kecil

  
**- Goals**

Analisis kredit sangat penting dalam dunia keuangan, terutama bagi lembaga perbankan dan pemberi pinjaman. bertujuan untuk:
1.   **Prediksi dini pada calon debitur yang berpotensi gagal bayar** dimana analisis kredit dilakukan untuk menilai risiko dari calon debitur atau nasabah. Dengan prediksi nilai kredit pada masing-masing debitur. Sehingga pihak bank dapat memastikan bahwa mereka meminjamkan uang kepada individu atau perusahaan yang mampu membayar kembali pinjaman tersebut. Dengan analisis kredit yang baik, risiko kredit macet (non-performing loans) dapat diminimalisir.
2.   **Prediksi skor penilaian debitur**, analisis ini bertujuan untuk memahami sejauh mana kemampuan calon peminjam dalam membayar kembali pinjaman. Ini melibatkan analisis terhadap pendapatan, stabilitas keuangan, dan arus kas dari calon debitur. Penilaian ini membantu lembaga keuangan dalam menentukan besarnya pinjaman yang layak diberikan
3.  **Menurunkan jumlah debitur yang melakukan gagal bayar dan menyesuaikan kontrak terhadap calon debitur**, Berdasarkan hasil analisis kredit, lembaga keuangan dapat menyesuaikan kontrak dengan nasabah seperti tingkat suku bunga yang sesuai bagi calon debitur. Sehingga kontrak terhadap debitur seperti suku bunga dapat menyesuaikan debitur untuk menghindari potensi gagal bayar. Peminjam dengan profil risiko yang rendah biasanya mendapatkan suku bunga yang lebih rendah yang dapat memudahkan debitur melunaskan hutangnya, sedangkan yang berisiko tinggi mendapatkan suku bunga yang lebih tinggi yang akan dihindari bank.

**- Solution statements**
Menggunakan algoritma regresi dengan mengeksplore algortima regresi dengan hyperparameter default dari library sklearn yang dapat terukur baik menggunakan MSE, MAE dan R2

### 3. Data Understanding

Data yang digunakan bersumber [Kaggle](https://www.kaggle.com/datasets/laotse/credit-risk-dataset), data tersebut berisi profil nasabah pada lembaga keuangan/bank yang memiliki history kredit bank, berjumlah 32581 baris dengan 12 kolom. Dan berikut penjelasan kolom-kolom pada data berikut:

- person_age	= Age (Usia Nasabah)
- person_income	= Annual Income (Pendapatan per bulan) --> diasumsi kan menggunakan Dollar
- person_home_ownership	= Home ownership (Kepemilikan tempat tinggal)
- person_emp_length	= Employment length in years (Lama pengalaman pekerjaan dalam tahunan)
- loan_intent	= Loan intent (Kebutuhan peminjaman)
- loan_grade	= Loan grade (Tingkat peminjaman)
- loan_amnt	= Loan amount (Jumlah peminjaman)
- loan_int_rate	= Interest rate (Rate bunga)
- loan_status =	Loan status, 0 is non default 1 is default (Status peminjaman, 1 = gagal bayar dan 0 = sukses bayar)
- loan_percent_income	= Percent income (persen pendapatan untuk peminjaman)
- cb_person_default_on_file	= Historical default (rekap gagal bayar = 1, lunas = 0)
- cb_preson_cred_hist_length = 	Credit history length --> credit score, bigger meaning to bad score (nilai kredit, semakin besar semakin buruk nilai kredit tersebut)

##### 3.1 Mengambil sumber data

Sumber data menggunakan API dari Kaggle dan membuat variabel untuk mengkonversi data csv ke data frame (menggunakan pandas) untuk memudahkan dalam mengelola dan menganalisis data dalam format tabular.
"""

!kaggle datasets download -d laotse/credit-risk-dataset
!unzip credit-risk-dataset -d '/content/dataset'

import pandas as pd
data = pd.read_csv('/content/dataset/credit_risk_dataset.csv')
data.shape

data_prep = data.copy()
data_prep

"""##### 3. 2 Melihat nilai unik data kategorikal"""

columns = ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']
for i in columns:
    print('- column', i + '=', data_prep[i].unique())

"""##### 3. 3 Melihat jumlah data pada tiap kolom"""

data_prep.info()

"""##### 3. 4 Melihat nilai null"""

print('count null rows = ', data_prep.isnull().any(axis=1).sum())
data_null = pd.DataFrame(data={'count null': data_prep.isnull().sum()}).rename_axis('column name')
data_null

"""##### 3. 5 Melihat nilai yang duplikat"""

print('count row duplicated = ', data_prep.duplicated().sum())
duplicate_rows = data_prep[data_prep.duplicated()]
duplicate_rows

"""- Kondisi data terebut memiliki nilai kolom null sebanyak 3943 baris dan 165. Untuk mengurangi bias data maka dilakukan drop nilai null dan duplikat

- Berdasarkan tujuan yaitu predictive analytics maka dilakukan EDA berfokus pada data yang memiliki riwayat dan kredit gagal bayar

##### 3. 6 EDA-Melihat data pada riwayat gagal bayar

Data yang akan digunakan akan berfokus pada debitur yang memiliki riwayat gagal bayar. Step ini akan menganalisis apakah debitur yang memiliki riwayat gagal bayar memiliki potensi status kredit gagal bayar atau lunas?

- Melihat data yang memiliki riwayat gagal  bayar (kolom cb_person_default_on_file = Y) dan status kredit (kolom loan_status = 1 dan 0)
"""

hist_default_Y = data_prep[data_prep['cb_person_default_on_file'] == 'Y']
print(hist_default_Y.shape)
hist_default_Y['loan_status'].value_counts()

"""- Melihat data yang memiliki riwayat lunas (kolom cb_person_default_on_file = N) dan status kredit (kolom loan_status = 1 dan 0)

"""

hist_default_N = data_prep[data_prep['cb_person_default_on_file'] == 'N']
print(hist_default_N.shape)
hist_default_N['loan_status'].value_counts()

"""**- Berdasarkan data yang memiliki riwayat gagal bayar memiliki potensi kredit gagal bayar dan lunas, sedangkan yang memiliki riwayat lunas memiliki potensi lunas yang tinggi namun ada kemungkinan gagal bayar.**


**- Berdasarkan analisis sementara tersebut maka riwayat lunas dan memiliki potensi lunas data tersebut akan di drop (kolom cb_person_default_on_file = N dan loan_status = 0). Bertujuan mengurangi bias data yang memiliki profil kredit yang lunas.**

Data awal berjumlah 32581, didrop data riwayat lunas dan memiliki potensi kredit lunas sejumlah 21900. Data menjadi 10681
"""

histN_lsN = (data_prep['cb_person_default_on_file'] == 'N') & (data_prep['loan_status'] == 0)

# Drop the rows based on the mask
hist_default_Y = data_prep[~histN_lsN]
hist_default_Y

"""##### 3. 7 EDA -  Melihat data tingkat kredit (kolom loan_grade dari A - G) dan status kredit gagal bayar dan lunas (kolom loan_status = 0 dan 1)

Data saat ini berfokus pada profil kreditur yang memiliki riwayat gagal bayar dan potensi kredit gagal bayar. Pengklasifikasian tingkat kredit berdasarkan riwayat kredit, dimana debitur yang memiliki riwayat gagal bayar diklasifikasikan status kredit tidak aman. Step ini melihat apakah status loan_grade berpotensi kredit gagal bayar?

- Melihat tingkat kredit (kolom loan_grade A-G) dengan kondisi status kredit lunas (kolom loan_status = 0)
"""

loan_gr = hist_default_Y[hist_default_Y['loan_status']== 0]
loan_gr['loan_grade'].value_counts()

"""- Melihat tingkat kredit (kolom loan_grade A-G) dengan kondisi status kredit lunas (kolom loan_status = 1)

"""

loan_gr = hist_default_Y[hist_default_Y['loan_status']== 1]
loan_gr['loan_grade'].value_counts().sort_index()

"""**- Berdasarkan data yang memiliki riwayat kredit gagal bayar, data kredit yang gagal bayar terklasifikasi loan_grade : C, D ,E, F, G sedangkan kredit lunas terklasifikasi A, B, C, D, E, F, G**

**- Analisis sementara: loan_grade C, D, E, F, G berpotensi kredit gagal bayar, sehingga data loan_grade A dan B akan didrop. Bertujuan mengurangi bias data yang memiliki tingkat kredit aman**
"""

grade_safe = (hist_default_Y['loan_grade'].isin(['A', 'B'])) & (hist_default_Y['loan_status'] == 1)

# Drop the rows based on the mask
data_grade_risk = hist_default_Y[~grade_safe]
data_grade_risk

"""##### 3. 8 Membuang data yang tidak digunakan -> kolom data yang tidak digunakan, data null dan data duplikat

Setelah melakukan drop data yang memiliki riwayat lunas dan status kredit aman, pada step 3.4 dan 3.5 data terdapat null dan duplikat. Sehingga perlu untuk drop kondisi data yang berpotensi bias dan tidak digunakan. Yaitu drop data null dan duplikat juga dilakukan drop kolom yang tidak akan digunakan.
"""

data_prep = data_grade_risk.drop(columns=['loan_percent_income', 'loan_grade', 'loan_status', 'cb_person_default_on_file'])
data_prep

data_dropnull = data_prep.dropna()
data_clean = data_dropnull.drop_duplicates()
data_clean

"""##### 3. 9 Mengecek nilai null dan duplikat dan melihat jumlah data masing-masing kolom not null dan tidak terduplikat"""

print('count row duplicated = ', data_clean.duplicated().sum())
print('count null rows = ', data_clean.isnull().any(axis=1).sum())

data_clean.info()

"""##### 3. 10 Memperbaiki nama kolom dan menyesuaikan urutan kolom

Tujuan step ini untuk mempermudah dalam analisis data tabular secara penamaan kolom dan urutan kolom

- persona nasabah: person_age, experience_work, person_income, home_status
- variabel kredit: loan_amount, loan_int_rate, loan_intent, credit_score
"""

data_clean = data_clean.rename(columns={'person_home_ownership': 'home_status', 'person_emp_length': 'experience_work', 'loan_amnt': 'loan_amount', 'cb_person_cred_hist_length' : 'credit_score'  })
data_clean = data_clean[['person_age', 'experience_work', 'person_income', 'home_status', 'loan_amount', 'loan_int_rate', 'loan_intent', 'credit_score' ]]
data_clean

"""##### 3. 11 Melihat rentang data dengan analisis statistik deskriptif"""

data_clean.describe()

"""**- dari deskripsi nilai max dan Q3 person_age 70 dan experience work 123, nilai-nilai tersebut memiliki persebaran kurang tepat pada kondisi kolom-kolom tersebut menunjukkan terdapat outlier sehingga yang diperlukan pembersihan data lagi**

##### 3. 12 Visualisasi data boxplot-> melihat outlier pada kolom age dan experience_work
"""

import matplotlib.pyplot as plt
import seaborn as sns
sns.boxplot(x=data_clean['person_age'], color='aqua')

sns.boxplot(x=data_clean['experience_work'], color='aqua')

"""##### 3. 13 Visualisasi data boxplot -> Membuang outlier dan melihat data tanpa outlier

"""

Q1 = data_clean['person_age'].quantile(0.25)
Q3 = data_clean['person_age'].quantile(0.75)
IQR=Q3-Q1
data_clean=data_clean[~((data_clean['person_age']<(Q1-1.5*IQR))|(data_clean['person_age']>(Q3+1.5*IQR)))]
# visualisasi outlier
sns.boxplot(x=data_clean['person_age'], color='teal')

Q1 = data_clean['experience_work'].quantile(0.25)
Q3 = data_clean['experience_work'].quantile(0.75)

data_clean=data_clean[~((data_clean['experience_work']<(Q1-1.5*IQR))|(data_clean['experience_work']>(Q3+1.5*IQR)))]

# visualisasi outlier
sns.boxplot(x=data_clean['experience_work'], color='teal')

data_clean.describe()

"""**Pada kolom person_age dan experience_work menampilkan nilai sesuai, nilai max kolom-kolom terlihat bukan nilai outlier**"""

data_clean.shape

"""##### 3. 14 Visualisasi Data Univariat- Kategorikal"""

marital_counts = data_clean['home_status'].value_counts()

colors = ['aquamarine', 'teal', 'lightgreen', 'green']
plt.figure(figsize=(10, 5.8))
plt.pie(marital_counts.values, labels=marital_counts.index, colors=colors,
autopct='%1.1f%%', shadow=True, startangle=140, wedgeprops=dict(width=0.6))
plt.axis('equal')
plt.show()

"""-> dominasi debitur memiliki rumah dengan tanpa kepemilikan yaitu kepemilikan sewa mencapai 61.6% dan hipotek sebesar 32.1%"""

loan = data_clean['loan_intent'].value_counts()
plt.bar(loan.index, loan.values, color='teal')
plt.xticks(rotation=45)
plt.xlabel('Type')
plt.ylabel('Count')
plt.title('Loan Intent')
plt.show()

"""- nasabah yang mengajukan kredit untuk memiliki kebutuhan paling tinggi adalah Medis namun juga disusul kebutuhan lain pendidikan. Dan paling rendah untuk kebutuhan perbaikan rumah

##### 3. 15 Visualisasi data Univariat -> Numerikal
"""

person_income = data_clean['person_income'].value_counts()
plt.scatter(person_income.index, person_income.values, color='teal', alpha=.5)
plt.xlabel('Person Income')
plt.ylabel('Count')
plt.title('Person Income Distribution')
plt.show()

loan_amount = data_clean['loan_amount'].value_counts()
plt.scatter(loan_amount.index, loan_amount.values, color='darkgreen', alpha=.5)
plt.xlabel('Loan Amount')
plt.ylabel('Count')
plt.title('Loan Amount Distribution')
plt.show()

data_clean.hist(bins=50, figsize=(15,10), color='teal', edgecolor='darkblue')
plt.tight_layout()
plt.show()

"""- Jumlah nasabah mengajukan kredit adalah debitur yang berumur rentaang 23-25 mencapai lebih dari 300. Banyak debitur mengajukan kredit yang masih memiliki pengalaman kerja < 5 tahun. Dan jumlah yng tinggi pada pendapatan debitur pertahun adalah < 200000

- Pada kolom credit_score sebagai target menunjukan data yang imbalance, sehingga step selanjutnya akan dilakukan balance data

##### 3. 16 Visualisasi data multivariat
"""

variabel_loan = data_clean.select_dtypes(include='object').columns.to_list()

for col in variabel_loan:
  sns.catplot(x=col, y="credit_score", kind="bar", hue = col, dodge=False, height = 4, aspect = 3,  data=data_clean, palette="crest", legend=False)
  plt.title("Rata-rata 'credit_score' Relatif terhadap - {}".format(col))

"""Berdasarkan color grade menunjukkan Ascending

- Pada kolom home_status, rata-rata credit_score menunjukan sama masing-masing home_status selain color grade terendah. Rentang home_status terhadap rata-rata credit score 5-6. Grade tertinggi yaitu grade kreditur memiliki kepemilikan rumah hipotik.
Dapat disimpulkan kolom home_status memiliki pengaruh kecil karna rata-rata yang tidak ada cenderung pada masing-masing nilai home_status

- Pada kolom loan_intent, rata-rata loan_intent menunjukan rata-rata credit score yang memiliki rentang sama yaitu 5-6. Rentang loan_intent terhadap rata-rata credit score 5-6. Grade tertinggi yaitu grade kreditur memiliki tujuan kredit untuk Medis.
Dapat disimpulkan kolom loan_intent memiliki pengaruh kecil karna rata-rata yang tidak ada cenderung pada masing-masing nilai loan_intent

"""

data_corr = data_clean.corr(numeric_only=True)

plt.figure(figsize=(10, 8))
sns.heatmap(data_corr, annot=True, cmap='crest', fmt='.2f', linewidths=.8)
plt.title('Random Data Heatmap')
plt.show()

"""- Tidak ada korelasi kuat selain 0.84 pada korelasi kuat antara person_age dan credit_score"""

sns.pairplot(data_clean, diag_kind='kde', hue='loan_intent', palette='crest')

"""- Tidak ada korelasi kuat pada kolom credit_score hanya memiliki korelasi kuat pada person_age dimana persebaran kearah kiri. Riwayat skor kredit semakin tinggi terhadap usia kreditur yang semakin tua
- Pada visualisasi korelasi tersebut yang memiliki korelasi kuat pada kolom target "credit_score" hanya kolom "person_age". Dimana algoritma yang akan digunakan adalah algoritma non-linear/berbasis jarak

### 4. Data Preparation

##### 4. 1 Labelling Data - melakukan label data kategorikal ke data angka

Untuk membangun model data input secara keseluruhan adalah numerik. Pada data kategorikal data type string perlu dikonversi ke numerik. Metode yang akan digunakan menggunakan Label Encoder dimana nilai kategorikal akan berupa numerik.
"""

data_use=data_clean.copy()

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
data_use['home_status_en'] = label_encoder.fit_transform(data_use['home_status'])
data_use['loan_intent_en'] = label_encoder.fit_transform(data_use['loan_intent'])

data_use

home_status = data_use.drop_duplicates(subset=['home_status'])
home_status[['home_status', 'home_status_en']].sort_values(by='home_status_en')

loan_intent = data_use.drop_duplicates(subset=['loan_intent'])
loan_intent[['loan_intent', 'loan_intent_en']].sort_values(by='loan_intent_en')

"""##### 4. 2 Data splitting -> Menggunakan rasio training dan testing 90:10.

Data akan displit menjadi training dan testing dengan jumlah
"""

from sklearn.model_selection import train_test_split
data_model = data_use[['credit_score', 'person_age', 'experience_work', 'person_income', 'loan_amount', 'loan_int_rate',  'home_status_en', 'loan_intent_en']]
X = data_model.drop(["credit_score"],axis =1)
y = data_model["credit_score"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 123)
X_train_sc, X_test_sc = X_train.copy(), X_test.copy()

print('Total data of sample in train dataset: ', X_train_sc.shape[0])
print('Total data of sample in test dataset: ', X_test_sc.shape[0])

"""##### 4. 3 Balancing Data -> Menggunakan teknik SMOTE

Berdasarkan visualisasi step 3. 15 persebaran data pada nilai-nilai kolom credit_score tidak seimbang (imbalance data) dimana, skor nilai yang semakin tinggi menunjukkan jumlah data yang kecil. Project ini berfokus prediksi terhadap variabel yang memiliki nilai kredit yang tinggi untuk menghindari potensi debitur gagal bayar. **SMOTE** bekerja dengan cara menciptakan contoh sintetis baru dari kelas minoritas dengan mengambil dua atau lebih contoh minoritas yang ada dan menginterpolasi di antara mereka. Sehingga diperlukan teknik **SMOTE** pada data agar data debitur yang memiliki skor kredit tinggi dapat seimbang dengan lainnya.
"""

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_train_sc, y_train= smote.fit_resample(X_train_sc, y_train)
print(f'Total data of sample in whole dataset: {len(X_train)}')
print(f'Total data of sample in train dataset after SMOTE: {len(X_train_sc)}')
print(f'Total data of sample in test dataset: {len(X_test_sc)}')

"""##### 4. 4 Feature Engineering - Standarisasi data

Tujuan standarisasi adalah proses transformasi data agar memiliki rata-rata (mean) 0 dan standar deviasi 1. Step ini membentuk data lebih seragam, sehingga setiap fitur memiliki skala yang sebanding.
"""

from sklearn.preprocessing import StandardScaler

numerical_features = ['person_age', 'experience_work', 'person_income', 'loan_amount', 'loan_int_rate', 'home_status_en', 'loan_intent_en'] #, 'loan_amount', 'loan_percent_income', 'previous_default_en'
scaler = StandardScaler()
scaler.fit(X_train_sc[numerical_features])
X_train_sc[numerical_features] = scaler.transform(X_train_sc.loc[:, numerical_features])
X_train_sc[numerical_features].head(5)

X_train_sc[numerical_features].describe().round(4)

X_test_sc.loc[:, numerical_features] = scaler.transform(X_test_sc[numerical_features])

"""### 5. Modelling menggunakan algoritma Machine Learning Regressor

Berdasarkan pemrosesan data, data tidak ada korelasi kuat antar masing-masing fitur sehingga implementasi algortima akan menggunakan Regresi Non-Linear dan jarak untuk mengenali fitur satu antar lainnya, parameter algortima akan mengggunakan default dari library sklearn
1. KNN : KNN bekerja berbasis jarak dengan membandingkan jarak satu sampel ke sampel pelatihan lain dengan memilih sejumlah k tetangga terdekat (dengan k adalah sebuah angka positif). Algoritma KNN akan mencari/membentuk kedekatan fitur satu dengan lainnya.
  - Pada implementasi akan menggunakan parameter:
    - K yaitu K=10, maka prediksi dari suatu data baru adalah rata-rata nilai target dari 10 titik data terdekat,

2. Gradient Boosting : Boosting bekerja dengan membangun model dari data latih dengan pengoptimalan dengan menggunakan loss function untuk meminimalisir kesalahan secara iteratif. Kemudian ia membuat model kedua yang bertugas memperbaiki kesalahan dari model pertama. Model ditambahkan sampai data latih terprediksi dengan baik atau telah mencapai jumlah maksimum model untuk ditambahkan.
  - Pada implementasi akan menggunakan parameter:
    - n_estimator: 300,  jumlah weak learners atau pohon keputusan yang akan digunakan,
    - learning_rate: 0.1 nilai yang lebih rendah seperti 0.01 atau 0.1 memberikan hasil yang lebih stabil tetapi memerlukan lebih banyak n_estimators, sehingga menggunakan 0.1 karena nilai n_estimator tidak banyak
    - max_depth = 3 menentukan kedalaman maksimum setiap pohon keputusan, semakin besar nilai ini, semakin kompleks pohonnya dan semakin tinggi potensi overfitting


3. AdaaBoost: AdaBoost adalah salah satu algoritma boosting dan bagging yang bekerja dengan cara membangun beberapa model yang lemah (weak learners, sering kali kombinasi algoritma Decision Trees sederhana) secara bertahap secara Bootstrap sampling dan agregrasi prediksi.
Setiap model baru berusaha memperbaiki kesalahan yang dibuat oleh model sebelumnya dengan memberikan bobot lebih besar pada kesalahan tersebut. Model akhir adalah kombinasi berbobot dari semua weak learners.
  - Pada implementasi akan menggunakan parameter:
    - learning_rate: 0.5 nilai yang lebih rendah seperti 0.01 atau 0.1 memberikan hasil yang lebih stabil tetapi memerlukan lebih banyak n_estimators, sehingga menggunakan 0.1 karena nilai n_estimator tidak banyak.
    - random_state: 55 mengontrol pengacakan data dan penentuan nilai awal sehingga hasilnya konsisten jika kode dijalankan kembali


4. HistGradient Boosting: HistGradientBoostingRegressor adalah varian dari Gradient Boosting yang lebih efisien dan dioptimalkan untuk dataset besar.
Algoritma ini mengubah data input menjadi bentuk histogram, yang kemudian digunakan untuk mempercepat proses pembagian dan pelatihan.
  - Pada implementasi akan menggunakan parameter:
    - learning_rate: 0.05 nilai yang lebih rendah seperti 0.01 atau 0.1 memberikan hasil yang lebih stabil tetapi memerlukan lebih banyak n_estimators, sehingga menggunakan 0.1 karena nilai n_estimator tidak banyak.
    - max_iter: 500 menentukan jumlah weak learners atau iterasi boosting yang akan dibangun.
    - max_depth:5 menentukan kedalaman maksimum dari setiap pohon sebagai cara lain untuk mengontrol kompleksitas pohon
    - min_samples_leaf: 10 menentukan jumlah minimum sampel yang diperlukan untuk membentuk sebuah daun.
    - max_bins: 255 menentukan jumlah maksimum bin yang digunakan untuk menyimpan data namun nilai yang lebih tinggi dapat meningkatkan resolusi split
    - l2_regularization: 0.1 menentukan kekuatan regularisasi L2 untuk mengurangi kompleksitas model dan menghindari overfitting
    - random_state: 64 mengontrol pengacakan data dan penentuan nilai awal sehingga hasilnya konsisten jika kode dijalankan kembali
    - early_stopping: True, untuk menghentikan pelatihan lebih awal jika performa tidak meningkat pada data validasi, jika disetel ke True, pelatihan akan berhenti ketika tidak ada perbaikan selama 10 iterasi; False berarti tidak ada penghentian awa
    - random_state: 42 mengontrol pengacakan data dan penentuan nilai awal sehingga hasilnya konsisten jika kode dijalankan kembali


5. DTR + Bagging: Bagging adalah menggabungkan hasil dari beberapa model dasar (base learners) yang dilatih pada subset acak dari data pelatihan.
  - Pada implementasi akan menggunakan parameter:
    - base_estimator: DecisionTreeRegressor default estimatornya
    - n_estimator yaitu 42,  jumlah weak learners atau pohon keputusan yang akan digunakan,
    - random_state: 64 mengontrol pengacakan data dan penentuan nilai awal sehingga hasilnya konsisten jika kode dijalankan kembali

6. Support Vector: SVR dapat menggunakan kernel non-linear seperti Radial Basis Function (RBF) untuk menangkap hubungan yang lebih kompleks.
  - Pada implementasi akan menggunakan parameter:
    - C:100 parameter unutk mengontrol trade-off antara minimization error pada data pelatihan dan kompleksitas model
    - epsilon:0.1  menentukan margin di sekitar prediksi di mana tidak ada penalti untuk kesalahan, contohnya jika epsilon diset ke 0.1, model akan mengabaikan kesalahan prediksi yang berada dalam jarak 0.1 dari nilai aktual.
    - kernel = 'rbf' Tipe kernel yang digunakan untuk mengubah data ke dalam bentuk yang memungkinkan untuk regresi non-linear
    - gamma:0.1 untuk mengatur parameter untuk kernel RBF, yang mengontrol jangkauan dari pengaruh titik pelatihan

7. Random Forest  metode ensemble dari banyak Decision Trees, yang bisa lebih kuat dalam memprediksi target dari data yang kompleks.
  - Pada implementasi akan menggunakan parameter:
    - n_estimator yaitu 50,  jumlah weak learners atau pohon keputusan yang akan digunakan,
    - max_depth = 16 menentukan kedalaman maksimum setiap pohon keputusan, semakin besar nilai ini, semakin kompleks pohonnya dan semakin tinggi potensi overfitting
    - random_state: 55 mengontrol pengacakan data dan penentuan nilai awal sehingga hasilnya konsisten jika kode dijalankan kembali

8. Decision Tree: Merupakan algortima non-linier dengan model pohon keputusan seperti Decision Tree Regressor mampu menangkap hubungan non-linier antar variabel.
  - Pada implementasi akan menggunakan parameter:
    - max_depth = 4 menentukan kedalaman maksimum setiap pohon keputusan, semakin besar nilai ini, semakin kompleks pohonnya dan semakin tinggi potensi overfitting
    - min_samples_split: 5 jumlah minimum sampel yang diperlukan untuk membagi suatu node
    - min_samples_leaf: 3 jumlah minimum sampel yang diperlukan untuk berada dalam suatu daun
    - max_features: None menentukan jumlah fitur yang akan dipertimbangkan saat mencari pemisahan terbaik, default berati menggunakan semua fitur
"""

#1 KNN
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

knn = KNeighborsRegressor(n_neighbors=10)

#2 GBR
from sklearn.ensemble import GradientBoostingRegressor
gbr = GradientBoostingRegressor(n_estimators=300, learning_rate=0.1, max_depth=3)

#3 Adaaptive Boosting
from sklearn.ensemble import AdaBoostRegressor
adaboost = AdaBoostRegressor(learning_rate=0.05, random_state=55)

#4 HGBR
from sklearn.ensemble import HistGradientBoostingRegressor
hgbr = HistGradientBoostingRegressor(
    max_iter=500,
    learning_rate=0.05,
    max_depth=5,
    min_samples_leaf=10,
    max_bins=255,
    l2_regularization=0.1,
    early_stopping=True,
    random_state=42
)

#5 DTR+Bagging
from sklearn.ensemble import BaggingRegressor
from sklearn.tree import DecisionTreeRegressor
br =  BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=42, random_state=64)

#6 SVR

from sklearn.svm import SVR
svr = SVR(kernel='rbf', C=100, epsilon=0.1, gamma=0.1)

#7 RF

from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(n_estimators=50, max_depth=16, random_state=55)

#8 DT
from sklearn.tree import DecisionTreeRegressor
dtr = DecisionTreeRegressor(
    max_depth=4,
    min_samples_split=5,
    min_samples_leaf=3,
    max_features=None
)

"""### 6. Evaluasi model -> MSE, MAE, R2

Evaluasi hasil model Regresi Non-linier akan menggunakan:

- MSE: Mengukur rata-rata dari kuadrat selisih antara nilai aktual dan nilai prediksi. Nilai MSE yang lebih rendah menunjukkan model yang lebih baik
- MAE: Mengukur rata-rata dari nilai absolut selisih antara nilai aktual dan nilai prediksi. Nilai MAE rendah akan menunjukan nilai kesalahan yang rendah.
- R2: Mengukur seberapa baik model memprediksi variasi dalam data. Nilainya berkisar antara 0 dan 1. Nilai R² yang lebih mendekati 1 menunjukkan model yang lebih baik.
"""

# Buat variabel mse yang isinya adalah dataframe nilai mse data train dan test pada masing-masing algoritma
eval_model = pd.DataFrame(columns=['train_mse', 'test_mse', 'train_mae', 'test_mae', 'train_r2', 'test_r2'], index=['knn', 'rf', 'adaboost', 'gbr', 'br','hgbr','svr','dtr'])

# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'knn':knn, 'rf':rf, 'adaboost':adaboost, 'gbr':gbr, 'br':br,'hgbr':hgbr,'svr':svr,'dtr':dtr}

# Pelatihan data ke model#
for name, model in model_dict.items():
  model.fit(X_train_sc, y_train)

# Hitung MSE, MAE, R2 masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
    eval_model.loc[name, 'train_mse'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train_sc))
    eval_model.loc[name, 'test_mse'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test_sc))
    eval_model.loc[name, 'train_mae'] = mean_absolute_error(y_true=y_train, y_pred=model.predict(X_train_sc))
    eval_model.loc[name, 'test_mae'] = mean_absolute_error(y_true=y_test, y_pred=model.predict(X_test_sc))
    eval_model.loc[name, 'train_r2'] = r2_score(y_true=y_train, y_pred=model.predict(X_train_sc))
    eval_model.loc[name, 'test_r2'] = r2_score(y_true=y_test, y_pred=model.predict(X_test_sc))

eval_model = eval_model.sort_values(by='test_mse', ascending=True)
eval_model

mse = eval_model[['train_mse', 'test_mse']]
mae = eval_model[['train_mae', 'test_mae']]
r2 = eval_model[['train_r2', 'test_r2']]
fig, axs = plt.subplots(1, 3, figsize=(18, 5))

mae.sort_values(by='test_mae').plot(kind='barh', ax=axs[0], zorder=3)
axs[0].grid(zorder=0)
axs[0].tick_params(axis='x', rotation=45)

mse.sort_values(by='test_mse').plot(kind='barh', ax=axs[1], zorder=3)
axs[1].grid(zorder=0)

r2.sort_values(by='test_r2', ascending=False).plot(kind='barh', ax=axs[2], zorder=3)
axs[2].grid(zorder=0)

prediksi = X_test_sc.loc[:].copy()
pred_dict = {'y_true':y_test[:]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)

predict = pd.DataFrame(pred_dict)#.sort_values(by='y_true', ascending=True)
predict

predict[['prediksi_br', 'y_true']].head(20)

"""Berdasarkan Nilai Evaluasi Matriks nilai MSE, MAE dan R2 training masing-masing algoritma:
1. Adaptive Boosting: MSE Train 2.884691, MAE Train 1.426624, R2 Train 0.86425
2. DecisionTree: MSE Train 2.834147, MAE Train 1.409014, R2 Train 0.866628
3. GradientBoost: MSE Train 2.107652, MAE Train 1.205095, R2 Train 0.900816
4. Random Forest: MSE Train 0.533179, MAE Train 0.521623, R2 Train 0.974909
5. HistGradientBoost: MSE Train 1.34575, MAE Train 0.930221, R2 Train 0.936671
6. **Bagging Regressor: MSE Train 0.135168, MAE Train 0.249788, R2 Train 0.993639**
7. Support Vector Regressor: MSE Train 2.991074, MAE Train 1.264286, R2 Train 0.859244
8. KNearestNeighbor: MSE Train 2.041922, MAE Train 0.997013, R2 Train 0.90391

Nilai MSE, MAE dan R2 testing masing-masing algoritma:
    
1. Adaptive Boosting: MSE Test 1.749497, MAE Test 1.069593, R2 Test 0.84433
2. DecisionTree: MSE Test 1.803272, MAE Test 1.07573, R2 Test 0.839545
3. GradientBoost: MSE Test 1.93315, MAE Test 1.110244, R2 Test 0.827988
4. Random Forest: MSE Test 1.964606, MAE Test 1.12127, R2 test 0.825189
5. HistGradientBoost: MSE Test 2.004605, MAE Test 1.129062, R2 Test 0.82163
6. **Bagging Regressor: MSE Test 2.073779, MAE Test 1.148535, R2 test 0.815475**
7. Support Vector Regressor: MSE Test 3.948882, MAE Test 1.516849, R2 Test 0.648629
8. KNearestNeighbor: MSE Test 4.054033, MAE Test 1.550302, R2 Test 0.639272

Hasil evaluasi matriks -> data prediksi yang digunakan merupakan data continue dan bersifat regresi dengan nilai mse dan mae lebih rendah menunjukkan nilai kesalahan lebih rendah dan R2 yang lebih tinggi menunjukkan model memiliki varians banyak dan model mendekati kesalahan rendah.

1. Bagging Regressor menunjukkan kinerja terbaik di data pelatihan dengan MSE terendah (0.135168), MAE terendah (0.249788), dan R² tertinggi (0.993639). Namun, di data pengujian performanya menurun, tetapi masih relatif baik.

2. Random Forest juga memiliki kinerja baik, dengan R² yang tinggi (0.974909) di pelatihan, tetapi sedikit lebih rendah di pengujian (0.825189).

3. Gradient Boost dan K-Nearest Neighbor menunjukkan kinerja yang rendah di pelatihan maupun pengujian.

4. Support Vector Regressor dan K-Nearest Neighbor memiliki kinerja terburuk di pengujian dan R² yang jauh lebih rendah.

**Model dengan evaluasi matriks diatas adalah: implementasi algortima br(BaggingRegressor)**

Hasil pengembangan/proyek ini dapat menjawab probelm statement yang dijelaskan pada Business Undestanding sebagai berikut:

  1. Analisis resiko kredit merupakan langkah penting untuk mengurangi debitur gagal bayar dimana pihak bank dapat menyetujui kredit atau menyesuaikan kontrak kredit dengan nasabah yang berpotensi gagal bayar. Dengan metode sederhana  CRISP-DM (Cross-Industry Standard Process for Data Mining) yaitu melakukan proses analisis pada data history/profil kreditur bank tersebut, melakukan teknik-teknik feature engineering bertujuan untuk mengkonversi data-data agar bisa diproses model algoritma dan untuk menentukan jenis algoritma predictive analytics pada skor nilai resiko kredit berdasarkan evaluasi matriks. Model predictive analytics yang sudah dilatih dan ditest inilah dapat memprediksi skor kredit untuk menilai kemampuan debitur terhadap kreditnya.

  2. Berdasarkan proses analisis data history bank menunjukkan korelasi yang sedikit pada latar belakang persona debitur dan variabel kredit yang diajukan. Hal tersebut mengindikasikan debitur mengajukan kredit dengan latar belakang persona apa saja dan kebutuhan yang beragam. Sehingga hal ini sulit jika dilakukan analisis dan prediksi manual pada resiko kredit. Solusi permasalahan tersebut adalah menggunakan prediksi skor kredi untuk analisis resiko kredit dengan pendekatan algoritma Machine Learning Regresi Non-Linier.

  3. Berdasarkan pemrosesan CRISP-DM hasil test MAE, MSE, R2 maka algoritma **Bagging Regressor ** menunjukkan prediksi dengan target memiliki nilai eror yang minimal. Dimana prediksi dapat membantu menentukan skor dari persona dan kategori kredit yang akan diajukan debitur. Sehingga pihak bank dapat menganalisis potensi dari calon debitur dan dapat meminimalisir debitur yang berpotensi gagal bayar. Dengan prediksi skor kredit pada calon debitur berdasarkan latar belakang persona debitur dan variabel kredit yang diajukan dapat membantu salah satu penilaian calon debitur terhadap pihak bank.

Poin-poin diatas dapat menjawab goals dimana pihak bank dapat prediksi dini untuk skor penilaian debitur. Dan juga dapat menyesuaikan bunga atau kontrak kredit berdasarkan prediksi dini skor kredit sehingga dapat meminimalisir debitur yang berpotensi gagal bayar.

Solusi proses pengembangan/penilitian yang diterapkan projek ini berdampak besar pada sektor financial dan banking dimana prediksi skor kredit dapat sebagai salah satu parameter penilaian aktivitas keuangan nasabah terhadap bank/lembaga keuangan. Selain itu prediksi pendekatan machine learning dapat dikembangkan berbagai parameter sektor keuangan misalnya prediksi kerugian pengeluaran, keuntungan income, prediksi harga, pengklasifikasian nasabah dsb.

#####***Test dengan data baru***
"""

import numpy as np
inp_pred = np.array([[29, 3, 45000, 20000, 3.45, 1, 3]])
inp_pred = pd.DataFrame(inp_pred, columns=numerical_features)
inp_pred

result_df = pd.concat([X_test, inp_pred])
result_df

prediksi_s = result_df

scaler = StandardScaler()
scaler.fit(result_df)
prediksi_s = scaler.transform(result_df)
prediksi_s = pd.DataFrame(prediksi_s,  columns=numerical_features)
prediksi_s

import warnings # Mengabaikan semua jenis UserWarning -> karna data baru terindikasi nama kolom yang tidak sesuai pelatihan/pengetesan
warnings.filterwarnings('ignore', category=UserWarning)


pred = {}
prediksi = prediksi_s.loc[[662]]
pred['prediksi_br'] = br.predict(prediksi).round(1)
df_pred = pd.DataFrame(pred, index=prediksi.index)
df_pred

